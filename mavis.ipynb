{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f308268",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytube moviepy opencv-python-headless pydub transformers sentence-transformers langchain faiss-cpu langchain-community torch numpy openai-whisper\n",
    "!pip install -U langchain-community\n",
    "!pip install langchain-huggingface\n",
    "!pip install gradio\n",
    "!pip install SpeechRecognition\n",
    "!pip install gtts\n",
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85612b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ENHANCED MULTI-AGENT SYSTEM WITH PROVEN ACCURACY PATTERNS\n",
    "# Combines your successful architecture with multi-agent intelligence\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import tempfile\n",
    "import pickle\n",
    "import asyncio\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "import subprocess\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import whisper\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from gtts import gTTS\n",
    "import gradio as gr\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED DATA INGESTION - USING YOUR PROVEN PATTERN\n",
    "# ===================================================================\n",
    "\n",
    "class ProvenPatternIngestion:\n",
    "    \"\"\"Data ingestion using your successful architecture patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use your exact configuration\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # Create directories like your approach\n",
    "        self.setup_directories()\n",
    "        logger.info(\"ProvenPatternIngestion initialized\")\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Setup directories exactly like your approach\"\"\"\n",
    "        for dir_name in [\"uploads\", \"frames\", \"audio\", \"faiss_index\"]:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    def save_uploaded_video(self, video_bytes):\n",
    "        \"\"\"Your exact video saving approach\"\"\"\n",
    "        try:\n",
    "            video_path = os.path.join(\"uploads\", f\"{uuid.uuid4().hex}.mp4\")\n",
    "            with open(video_path, \"wb\") as f:\n",
    "                f.write(video_bytes)\n",
    "            return video_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Video save failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_audio_improved(self, video_path):\n",
    "        \"\"\"Enhanced audio extraction with ffmpeg fallback\"\"\"\n",
    "        try:\n",
    "            # Try your original approach first\n",
    "            from moviepy.editor import VideoFileClip\n",
    "            with VideoFileClip(video_path) as clip:\n",
    "                if clip.audio is None:\n",
    "                    return None\n",
    "                \n",
    "                audio_path = os.path.join(\"audio\", f\"{uuid.uuid4().hex}_audio.wav\")\n",
    "                clip.audio.write_audiofile(audio_path, logger=None, verbose=False)\n",
    "                return audio_path\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"MoviePy extraction failed: {e}, trying ffmpeg...\")\n",
    "            \n",
    "            # Fallback to ffmpeg\n",
    "            try:\n",
    "                audio_path = os.path.join(\"audio\", f\"{uuid.uuid4().hex}_audio.wav\")\n",
    "                cmd = [\n",
    "                    'ffmpeg', '-i', video_path,\n",
    "                    '-vn', '-acodec', 'pcm_s16le', \n",
    "                    '-ar', '16000', '-ac', '1',\n",
    "                    '-y', audio_path\n",
    "                ]\n",
    "                subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "                return audio_path\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"FFmpeg extraction also failed: {e2}\")\n",
    "                return None\n",
    "    \n",
    "    def process_video_with_proven_pattern(self, video_bytes):\n",
    "        \"\"\"Process video using your proven successful pattern\"\"\"\n",
    "        \n",
    "        if not video_bytes:\n",
    "            return \"Please upload a file first.\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Save video (your approach)\n",
    "            video_path = self.save_uploaded_video(video_bytes)\n",
    "            if not video_path:\n",
    "                return \"Failed to save video\"\n",
    "            \n",
    "            # Step 2: Extract audio (enhanced)\n",
    "            audio_path = self.extract_audio_improved(video_path)\n",
    "            if not audio_path:\n",
    "                return \"Audio extraction failed\"\n",
    "            \n",
    "            # Step 3: Transcribe audio (your approach)\n",
    "            try:\n",
    "                transcription_result = self.whisper_model.transcribe(audio_path)\n",
    "                transcription = transcription_result[\"text\"].strip()\n",
    "                \n",
    "                # Store additional metadata for agents\n",
    "                transcription_metadata = {\n",
    "                    \"full_result\": transcription_result,\n",
    "                    \"confidence\": np.mean([seg.get(\"avg_logprob\", -1) for seg in transcription_result.get(\"segments\", [])]),\n",
    "                    \"language\": transcription_result.get(\"language\", \"en\")\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Transcription failed: {e}\")\n",
    "                return \"Transcription failed\"\n",
    "            \n",
    "            # Step 4: Extract frames (your approach with enhancements)\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "                interval = fps * 2  # 2 seconds interval like your approach\n",
    "                frame_paths = []\n",
    "                frame_count = 0\n",
    "                \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    if frame_count % interval == 0:\n",
    "                        frame_path = os.path.join(\"frames\", f\"frame_{uuid.uuid4().hex}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                        frame_paths.append(frame_path)\n",
    "                    frame_count += 1\n",
    "                \n",
    "                cap.release()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Frame extraction failed: {e}\")\n",
    "                return \"Frame extraction failed\"\n",
    "            \n",
    "            # Step 5: Create documents using your EXACT pattern but with agent enhancements\n",
    "            documents = []\n",
    "            \n",
    "            # Primary transcript document (your successful approach)\n",
    "            transcript_doc = Document(\n",
    "                page_content=transcription,\n",
    "                metadata={\n",
    "                    \"type\": \"transcript\", \n",
    "                    \"audio_path\": audio_path,\n",
    "                    \"confidence\": transcription_metadata[\"confidence\"],\n",
    "                    \"language\": transcription_metadata[\"language\"],\n",
    "                    \"word_count\": len(transcription.split()),\n",
    "                    \"content_priority\": \"high\"  # For agent prioritization\n",
    "                }\n",
    "            )\n",
    "            documents.append(transcript_doc)\n",
    "            \n",
    "            # Frame documents (your approach with enhancements)\n",
    "            for i, fp in enumerate(frame_paths):\n",
    "                # Store frame as base64 for agents\n",
    "                frame = cv2.imread(fp)\n",
    "                _, buffer = cv2.imencode('.jpg', frame)\n",
    "                frame_b64 = base64.b64encode(buffer).decode('utf-8')\n",
    "                \n",
    "                frame_doc = Document(\n",
    "                    page_content=f\"Visual content from frame {os.path.basename(fp)}\",\n",
    "                    metadata={\n",
    "                        \"type\": \"frame\",\n",
    "                        \"path\": fp,\n",
    "                        \"frame_b64\": frame_b64,\n",
    "                        \"timestamp\": i * 2,  # Approximate timestamp\n",
    "                        \"frame_index\": i,\n",
    "                        \"content_priority\": \"medium\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(frame_doc)\n",
    "            \n",
    "            # Step 6: Create vector store (your exact approach)\n",
    "            try:\n",
    "                vector_db = FAISS.from_documents(documents, self.embedding_model)\n",
    "                vector_db.save_local(\"faiss_index\")\n",
    "                \n",
    "                # Save additional metadata for agents\n",
    "                processing_metadata = {\n",
    "                    \"video_path\": video_path,\n",
    "                    \"audio_path\": audio_path,\n",
    "                    \"transcription\": transcription,\n",
    "                    \"transcription_metadata\": transcription_metadata,\n",
    "                    \"frame_count\": len(frame_paths),\n",
    "                    \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                    \"total_documents\": len(documents)\n",
    "                }\n",
    "                \n",
    "                with open(\"faiss_index/processing_metadata.json\", \"w\") as f:\n",
    "                    json.dump(processing_metadata, f, indent=2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"FAISS index creation failed: {e}\")\n",
    "                return \"Index creation failed\"\n",
    "            \n",
    "            return f\"Processing completed successfully! Transcription: '{transcription[:100]}...' | Frames: {len(frame_paths)} | Documents: {len(documents)}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical processing failed: {e}\")\n",
    "            return f\"Critical error: {str(e)}\"\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED RETRIEVAL AGENT - USING YOUR SUCCESSFUL PATTERNS\n",
    "# ===================================================================\n",
    "\n",
    "class EnhancedRetrievalAgent:\n",
    "    \"\"\"Retrieval agent that uses your proven context building pattern\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.agent_id = \"enhanced_retrieval_agent\"\n",
    "        logger.info(\"EnhancedRetrievalAgent initialized\")\n",
    "    \n",
    "    def retrieve_with_proven_pattern(self, vector_store: FAISS, query: str, k: int = 5) -> Tuple[List, str]:\n",
    "        \"\"\"Retrieve using your proven approach but with intelligent enhancements\"\"\"\n",
    "        try:\n",
    "            # Your exact retrieval approach\n",
    "            docs = vector_store.similarity_search(query, k=k)\n",
    "            \n",
    "            if not docs:\n",
    "                return [], \"No relevant information found for your query.\"\n",
    "            \n",
    "            # Build context using your EXACT successful pattern\n",
    "            context = \"Retrieved Information:\\n\"\n",
    "            frames = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                if doc.metadata[\"type\"] == \"transcript\":\n",
    "                    # Your exact pattern for transcript\n",
    "                    context += f\"- Transcript snippet: '{doc.page_content}'\\n\"\n",
    "                elif doc.metadata[\"type\"] == \"frame\":\n",
    "                    # Your exact pattern for frames\n",
    "                    context += f\"- A visual frame was identified related to the query.\\n\"\n",
    "                    \n",
    "                    # Collect frame for display (enhancement)\n",
    "                    if \"frame_b64\" in doc.metadata:\n",
    "                        try:\n",
    "                            img_bytes = base64.b64decode(doc.metadata[\"frame_b64\"])\n",
    "                            frame_np = np.frombuffer(img_bytes, np.uint8)\n",
    "                            frame_image = cv2.imdecode(frame_np, cv2.IMREAD_COLOR)\n",
    "                            frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "                            frames.append(frame_image)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Frame processing failed: {e}\")\n",
    "            \n",
    "            return {\n",
    "                \"context\": context,\n",
    "                \"retrieved_docs\": docs,\n",
    "                \"frames\": frames[:3],  # Limit to 3 frames like your approach\n",
    "                \"success\": True\n",
    "            }, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Retrieval failed: {e}\")\n",
    "            return [], f\"Failed to retrieve documents: {str(e)}\"\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED ANSWER AGENT - USING YOUR PROVEN LLM PATTERN\n",
    "# ===================================================================\n",
    "\n",
    "class EnhancedAnswerAgent:\n",
    "    \"\"\"Answer agent using your proven LLM interaction pattern\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        # Use your exact LLM configuration but with current model\n",
    "        self.llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key)\n",
    "        self.agent_id = \"enhanced_answer_agent\"\n",
    "        logger.info(\"EnhancedAnswerAgent initialized\")\n",
    "    \n",
    "    async def generate_answer_with_proven_pattern(self, query: str, retrieval_result: Dict) -> Dict:\n",
    "        \"\"\"Generate answer using your exact successful prompt pattern\"\"\"\n",
    "        \n",
    "        try:\n",
    "            context = retrieval_result[\"context\"]\n",
    "            \n",
    "            # Your EXACT prompt pattern that works\n",
    "            prompt = f\"Based on the following retrieved information from a video, provide a concise and helpful answer to the user's query.\\n\\nUser Query: \\\"{query}\\\"\\n\\n{context}\\n\\nSynthesized Answer:\"\n",
    "            \n",
    "            # Generate response\n",
    "            logging.info(\"Invoking LLM to generate summary...\")\n",
    "            generated_text = self.llm.invoke(prompt)\n",
    "            \n",
    "            if not generated_text:\n",
    "                raise ValueError(\"LLM returned an empty response.\")\n",
    "            \n",
    "            logging.info(f\"LLM generated text: {generated_text}\")\n",
    "            \n",
    "            return {\n",
    "                \"answer\": generated_text,\n",
    "                \"success\": True,\n",
    "                \"context_used\": context,\n",
    "                \"source_count\": len(retrieval_result[\"retrieved_docs\"])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Answer generation failed: {e}\")\n",
    "            return {\n",
    "                \"answer\": f\"LLM failed to process retrieved results: {str(e)}\",\n",
    "                \"success\": False,\n",
    "                \"context_used\": \"\",\n",
    "                \"source_count\": 0\n",
    "            }\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED AUDIO AGENT - USING YOUR PROVEN TTS PATTERN\n",
    "# ===================================================================\n",
    "\n",
    "class EnhancedAudioAgent:\n",
    "    \"\"\"Audio agent using your proven TTS pattern with enhancements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"enhanced_audio_agent\"\n",
    "        logger.info(\"EnhancedAudioAgent initialized\")\n",
    "    \n",
    "    def generate_audio_with_proven_pattern(self, text: str) -> Tuple[str, Optional[str]]:\n",
    "        \"\"\"Generate audio using your exact successful TTS pattern\"\"\"\n",
    "        \n",
    "        try:\n",
    "            logging.info(\"Generating audio file with gTTS...\")\n",
    "            \n",
    "            # Your exact TTS approach\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            audio_filename = f\"response_{uuid.uuid4().hex}.mp3\"\n",
    "            tts.save(audio_filename)\n",
    "            \n",
    "            logging.info(f\"Audio file saved as {audio_filename}\")\n",
    "            return audio_filename, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to generate audio file: {str(e)}\"\n",
    "            logging.error(error_message)\n",
    "            return None, error_message\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED ORCHESTRATION - COMBINING YOUR PATTERNS WITH AGENTS\n",
    "# ===================================================================\n",
    "\n",
    "class EnhancedOrchestrationAgent:\n",
    "    \"\"\"Orchestration that combines your successful patterns with multi-agent intelligence\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        # Initialize with your proven embedding approach\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize enhanced agents\n",
    "        self.retrieval_agent = EnhancedRetrievalAgent(self.embeddings)\n",
    "        self.answer_agent = EnhancedAnswerAgent(api_key)\n",
    "        self.audio_agent = EnhancedAudioAgent()\n",
    "        \n",
    "        # System state\n",
    "        self.vector_store = None\n",
    "        self.processing_metadata = None\n",
    "        \n",
    "        logger.info(\"EnhancedOrchestrationAgent initialized\")\n",
    "    \n",
    "    def load_proven_pattern_data(self) -> Tuple[bool, str]:\n",
    "        \"\"\"Load data using your proven FAISS loading pattern\"\"\"\n",
    "        try:\n",
    "            # Your exact loading approach\n",
    "            self.vector_store = FAISS.load_local(\"faiss_index\", self.embeddings, allow_dangerous_deserialization=True)\n",
    "            \n",
    "            # Load additional metadata for enhanced agents\n",
    "            metadata_path = \"faiss_index/processing_metadata.json\"\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, \"r\") as f:\n",
    "                    self.processing_metadata = json.load(f)\n",
    "            \n",
    "            return True, \"Data loaded successfully using proven pattern\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            return False, f\"Failed to load data: {str(e)}\"\n",
    "    \n",
    "    async def query_with_proven_pattern(self, query: str) -> Dict:\n",
    "        \"\"\"Query processing using your proven successful pattern\"\"\"\n",
    "        \n",
    "        if not self.vector_store:\n",
    "            return {\n",
    "                \"frames\": [],\n",
    "                \"answer\": \"No data loaded. Please process a video first.\",\n",
    "                \"audio_path\": None,\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Retrieval using your proven pattern\n",
    "            retrieval_result, error = self.retrieval_agent.retrieve_with_proven_pattern(\n",
    "                self.vector_store, query\n",
    "            )\n",
    "            \n",
    "            if error:\n",
    "                return {\n",
    "                    \"frames\": [],\n",
    "                    \"answer\": f\"Error during retrieval: {error}\",\n",
    "                    \"audio_path\": None,\n",
    "                    \"success\": False\n",
    "                }\n",
    "            \n",
    "            # Step 2: Answer generation using your proven LLM pattern\n",
    "            answer_result = await self.answer_agent.generate_answer_with_proven_pattern(\n",
    "                query, retrieval_result\n",
    "            )\n",
    "            \n",
    "            # Step 3: Audio generation using your proven TTS pattern\n",
    "            audio_path, audio_error = self.audio_agent.generate_audio_with_proven_pattern(\n",
    "                answer_result[\"answer\"]\n",
    "            )\n",
    "            \n",
    "            final_answer = answer_result[\"answer\"]\n",
    "            if audio_error:\n",
    "                final_answer += f\"\\n\\n[WARNING: {audio_error}]\"\n",
    "            \n",
    "            return {\n",
    "                \"frames\": retrieval_result[\"frames\"],\n",
    "                \"answer\": final_answer,\n",
    "                \"audio_path\": audio_path,\n",
    "                \"success\": True,\n",
    "                \"context_used\": answer_result.get(\"context_used\", \"\"),\n",
    "                \"source_count\": answer_result.get(\"source_count\", 0)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query processing failed: {e}\")\n",
    "            return {\n",
    "                \"frames\": [],\n",
    "                \"answer\": f\"Query processing failed: {str(e)}\",\n",
    "                \"audio_path\": None,\n",
    "                \"success\": False\n",
    "            }\n",
    "\n",
    "# ===================================================================\n",
    "# GRADIO INTERFACE - COMBINING YOUR SUCCESSFUL UI WITH ENHANCEMENTS\n",
    "# ===================================================================\n",
    "\n",
    "def create_enhanced_interface():\n",
    "    \"\"\"Create interface that combines your successful patterns with multi-agent enhancements\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    ingestion = ProvenPatternIngestion()\n",
    "    orchestrator = None\n",
    "    \n",
    "    def setup_system(api_key):\n",
    "        nonlocal orchestrator\n",
    "        try:\n",
    "            if not api_key:\n",
    "                return \"Please enter your API key\"\n",
    "            orchestrator = EnhancedOrchestrationAgent(api_key)\n",
    "            success, message = orchestrator.load_proven_pattern_data()\n",
    "            return message\n",
    "        except Exception as e:\n",
    "            return f\"Setup failed: {str(e)}\"\n",
    "    \n",
    "    def process_video_interface(video_file):\n",
    "        if not video_file:\n",
    "            return \"Please upload a video file\"\n",
    "        \n",
    "        with open(video_file, \"rb\") as f:\n",
    "            video_bytes = f.read()\n",
    "        \n",
    "        result = ingestion.process_video_with_proven_pattern(video_bytes)\n",
    "        return result\n",
    "    \n",
    "    async def query_interface(query):\n",
    "        if not orchestrator:\n",
    "            return [], \"Please setup the system first\", None\n",
    "        \n",
    "        result = await orchestrator.query_with_proven_pattern(query)\n",
    "        return result[\"frames\"], result[\"answer\"], result[\"audio_path\"]\n",
    "    \n",
    "    with gr.Blocks(title=\"Enhanced MAVIS with Proven Patterns\") as app:\n",
    "        gr.Markdown(\"# Enhanced Multi-Agent System with Proven Accuracy Patterns\")\n",
    "        gr.Markdown(\"Combines your successful architecture with multi-agent intelligence\")\n",
    "        \n",
    "        # Setup section\n",
    "        with gr.Row():\n",
    "            api_key_input = gr.Textbox(label=\"Google API Key\", type=\"password\")\n",
    "            setup_btn = gr.Button(\"Setup System\", variant=\"primary\")\n",
    "            setup_status = gr.Textbox(label=\"Setup Status\")\n",
    "        \n",
    "        # Processing section (your proven UI pattern)\n",
    "        with gr.Row():\n",
    "            video_input = gr.File(label=\"Upload Video\")\n",
    "            process_btn = gr.Button(\"Process Video\", variant=\"primary\")\n",
    "            process_status = gr.Textbox(label=\"Processing Status\", lines=3)\n",
    "        \n",
    "        # Query section (your proven UI pattern)\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(label=\"Enter your query about the video\")\n",
    "            query_btn = gr.Button(\"Get Answer\", variant=\"primary\")\n",
    "        \n",
    "        # Results section (your proven UI pattern)\n",
    "        with gr.Row():\n",
    "            result_frames = gr.Gallery(label=\"Relevant Frames\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            result_answer = gr.Textbox(label=\"LLM Generated Summary\", lines=6)\n",
    "            \n",
    "        with gr.Row():\n",
    "            result_audio = gr.Audio(label=\"Synthesized Audio Response\")\n",
    "        \n",
    "        # Event handlers\n",
    "        setup_btn.click(setup_system, inputs=[api_key_input], outputs=[setup_status])\n",
    "        process_btn.click(process_video_interface, inputs=[video_input], outputs=[process_status])\n",
    "        query_btn.click(query_interface, inputs=[query_input], outputs=[result_frames, result_answer, result_audio])\n",
    "    \n",
    "    return app\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = create_enhanced_interface()\n",
    "    app.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
